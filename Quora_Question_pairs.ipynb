{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>Astrology: I am a Capricorn Sun Cap moon and c...</td>\n",
       "      <td>I'm a triple Capricorn (Sun, Moon and ascendan...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>Should I buy tiago?</td>\n",
       "      <td>What keeps childern active and far from phone ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>How can I be a good geologist?</td>\n",
       "      <td>What should I do to be a great geologist?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>When do you use シ instead of し?</td>\n",
       "      <td>When do you use \"&amp;\" instead of \"and\"?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>Motorola (company): Can I hack my Charter Moto...</td>\n",
       "      <td>How do I hack Motorola DCX3400 for free internet?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "5   5    11    12  Astrology: I am a Capricorn Sun Cap moon and c...   \n",
       "6   6    13    14                                Should I buy tiago?   \n",
       "7   7    15    16                     How can I be a good geologist?   \n",
       "8   8    17    18                    When do you use シ instead of し?   \n",
       "9   9    19    20  Motorola (company): Can I hack my Charter Moto...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  \n",
       "5  I'm a triple Capricorn (Sun, Moon and ascendan...             1  \n",
       "6  What keeps childern active and far from phone ...             0  \n",
       "7          What should I do to be a great geologist?             1  \n",
       "8              When do you use \"&\" instead of \"and\"?             0  \n",
       "9  How do I hack Motorola DCX3400 for free internet?             0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Kaggle project for NLP (02-20-2019)\n",
    "\n",
    "# This is the first project in NLP.\n",
    "\n",
    "# We are a two-member group:\n",
    "# Ramesh Subedi\n",
    "# Ravi Dawar\n",
    "\n",
    "# It takes more than 12 hours to run this whole code.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import scipy # for scientific computing and technical computing. This library depends on numpy library.\n",
    "\n",
    "import re\n",
    "from string import punctuation\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "    \n",
    "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer\n",
    "from sklearn import linear_model, metrics\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import log_loss, roc_auc_score, confusion_matrix\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier  \n",
    "from xgboost.sklearn import XGBRegressor\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Source: https://www.kaggle.com/c/quora-question-pairs\n",
    "Data fields\n",
    "\n",
    "id -                   the id of a training set question pair\n",
    "qid1, qid2 -           unique ids of each question (only available in train.csv)\n",
    "question1, question2 - the full text of each question\n",
    "is_duplicate -         the target variable, set to 1 if question1 and question2 have essentially \n",
    "                       the same meaning, and 0 otherwise.\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_csv('/Users/rameshsubedi/Downloads/kaggle/train.csv') # read data\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'qid1', 'qid2', 'question1', 'question2', 'is_duplicate'], dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404290, 6)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find number of rows and columns\n",
    "df.shape # 404290 rows, 6 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>404290.000000</td>\n",
       "      <td>404290.000000</td>\n",
       "      <td>404290.000000</td>\n",
       "      <td>404290.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>202144.500000</td>\n",
       "      <td>217243.942418</td>\n",
       "      <td>220955.655337</td>\n",
       "      <td>0.369198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>116708.614502</td>\n",
       "      <td>157751.700002</td>\n",
       "      <td>159903.182629</td>\n",
       "      <td>0.482588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>101072.250000</td>\n",
       "      <td>74437.500000</td>\n",
       "      <td>74727.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>202144.500000</td>\n",
       "      <td>192182.000000</td>\n",
       "      <td>197052.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>303216.750000</td>\n",
       "      <td>346573.500000</td>\n",
       "      <td>354692.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>404289.000000</td>\n",
       "      <td>537932.000000</td>\n",
       "      <td>537933.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id           qid1           qid2   is_duplicate\n",
       "count  404290.000000  404290.000000  404290.000000  404290.000000\n",
       "mean   202144.500000  217243.942418  220955.655337       0.369198\n",
       "std    116708.614502  157751.700002  159903.182629       0.482588\n",
       "min         0.000000       1.000000       2.000000       0.000000\n",
       "25%    101072.250000   74437.500000   74727.000000       0.000000\n",
       "50%    202144.500000  192182.000000  197052.000000       0.000000\n",
       "75%    303216.750000  346573.500000  354692.500000       1.000000\n",
       "max    404289.000000  537932.000000  537933.000000       1.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Give basic statistics for columns that have numerical values\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 404290 entries, 0 to 404289\n",
      "Data columns (total 6 columns):\n",
      "id              404290 non-null int64\n",
      "qid1            404290 non-null int64\n",
      "qid2            404290 non-null int64\n",
      "question1       404289 non-null object\n",
      "question2       404288 non-null object\n",
      "is_duplicate    404290 non-null int64\n",
      "dtypes: int64(4), object(2)\n",
      "memory usage: 18.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id              0\n",
       "qid1            0\n",
       "qid2            0\n",
       "question1       1\n",
       "question2       2\n",
       "is_duplicate    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Check for missing values. Python by default replaces \n",
    "missing values by NaN.\"\"\"\n",
    "df.isnull().sum()\n",
    "# Found 1 NaN value in question1.\n",
    "# Found 2 NaN value in question2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404290, 6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404287, 6)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Removing NaN values \"\"\"\n",
    "df=df.dropna() # drop NaN values \n",
    "df.shape # there are now 404287 entries with no NaN values.\n",
    "# dropped 1 record from question1 and 2 records from question2; \n",
    "# hence overall 3 records dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id              0\n",
       "qid1            0\n",
       "qid2            0\n",
       "question1       0\n",
       "question2       0\n",
       "is_duplicate    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum() # Now there are no NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3692005926482919"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nume = df.is_duplicate.sum() # give me sum of is_duplicate==1\n",
    "#nume # we have 149263 duplicate questions given by default\n",
    "\n",
    "denom = df[\"is_duplicate\"].count() # give me total of is_duplicate.\n",
    "#denom   # 404287\n",
    "\n",
    "fraction_of_duplicate_to_total = nume/denom\n",
    "fraction_of_duplicate_to_total  \n",
    "# We have 36.92% of duplicate questions by default\n",
    "# Any number we get more than 37% in our work will be an achievement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the step by step guide to invest in share market in india?\n",
      "What is the step by step guide to invest in share market?\n",
      "\n",
      "What is the story of Kohinoor (Koh-i-Noor) Diamond?\n",
      "What would happen if the Indian government stole the Kohinoor (Koh-i-Noor) diamond back?\n",
      "\n",
      "How can I increase the speed of my internet connection while using a VPN?\n",
      "How can Internet speed be increased by hacking through DNS?\n",
      "\n",
      "Why am I mentally very lonely? How can I solve it?\n",
      "Find the remainder when [math]23^{24}[/math] is divided by 24,23?\n",
      "\n",
      "Which one dissolve in water quikly sugar, salt, methane and carbon di oxide?\n",
      "Which fish would survive in salt water?\n",
      "\n",
      "Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me?\n",
      "I'm a triple Capricorn (Sun, Moon and ascendant in Capricorn) What does this say about me?\n",
      "\n",
      "Should I buy tiago?\n",
      "What keeps childern active and far from phone and video games?\n",
      "\n",
      "How can I be a good geologist?\n",
      "What should I do to be a great geologist?\n",
      "\n",
      "When do you use シ instead of し?\n",
      "When do you use \"&\" instead of \"and\"?\n",
      "\n",
      "Motorola (company): Can I hack my Charter Motorolla DCX3400?\n",
      "How do I hack Motorola DCX3400 for free internet?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df.drop(['id', 'qid1', 'qid2'], axis=1, inplace=True) # drop the line with heading\n",
    "\n",
    "# Print 10 Question pairs (horizontal pairs like in df.head(10) above)\n",
    "a = 0\n",
    "for i in range(a,a+10):\n",
    "    print(df.question1[i]) # print first ith question\n",
    "    print(df.question2[i]) # print 2nd ith question\n",
    "    print()                # print empty line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of data points:  404287\n",
      "0    255024\n",
      "1    149263\n",
      "Name: is_duplicate, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# A plot to see how many is_duplicate is really duplicate (=1) and not really duplicate (=0)\n",
    "# Approximately 250k (with a tag=0) questions are NOT really duplicate, \n",
    "# and only about 150k (with a tag=1) seem to be duplicate. X-axis: Label distribution in data \n",
    "\n",
    "print(\"No of data points: \",df.shape[0])\n",
    "print(df[\"is_duplicate\"].value_counts()) # show 255024 0's and 149263 1's out of 404287 total is_duplicate counts.\n",
    "\n",
    "is_dup = df['is_duplicate'].value_counts()\n",
    "color = sns.color_palette()\n",
    "plt.figure(figsize=(8,4))\n",
    "\n",
    "sns.barplot(is_dup.index, is_dup.values, alpha=0.8, color=color[1])\n",
    "\n",
    "plt.ylabel('Number of Occurrences', fontsize=12)\n",
    "\n",
    "plt.xlabel('Is Duplicate', fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Clean up the text in questions.\n",
    "\n",
    "# This list is so extensive that we can look back at it as a reference when \n",
    "# needed for other regular expression related work.\n",
    "\n",
    "\n",
    "SPECIAL_TOKENS = {'non-ascii': 'non_ascii_word'}\n",
    "\n",
    "def clean(text, stem_words=True):\n",
    "    def pad_str(s):\n",
    "        return ' '+s+' '\n",
    "    \n",
    "    if pd.isnull(text):\n",
    "        return ''\n",
    "\n",
    "    # Empty question\n",
    "    \n",
    "    if type(text) != str or text=='':\n",
    "        return ''\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(\"\\'s\", \" \", text) # we have cases like \"Sam is\" or \"Sam's\" (i.e. his) these two cases aren't separable, I choose to compromise are kill \"'s\" directly\n",
    "    text = re.sub(\" whats \", \" what is \", text, flags=re.IGNORECASE) # replace whats by what is and ignore case\n",
    "    text = re.sub(\"\\'ve\", \" have \", text) # replace 've by have\n",
    "    text = re.sub(\"can't\", \"can not\", text) # replace can't by can not\n",
    "    text = re.sub(\"n't\", \" not \", text) # replace n't by not\n",
    "    text = re.sub(\"i'm\", \"i am\", text, flags=re.IGNORECASE) # replace i'm by i am and ignore case\n",
    "    text = re.sub(\"\\'re\", \" are \", text) # replace 're by are\n",
    "    text = re.sub(\"\\'d\", \" would \", text) # replace 'd by would\n",
    "    text = re.sub(\"\\'ll\", \" will \", text) # replace 'll by will\n",
    "    text = re.sub(\"e\\.g\\.\", \" eg \", text, flags=re.IGNORECASE) # replace e.g. by eg and ignore case\n",
    "    text = re.sub(\"b\\.g\\.\", \" bg \", text, flags=re.IGNORECASE) # replace b.g. by bg and ignore case\n",
    "    text = re.sub(\"(\\d+)(kK)\", \" \\g<1>000 \", text) \n",
    "    text = re.sub(\"e-mail\", \" email \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"(the[\\s]+|The[\\s]+)?U\\.S\\.A\\.\", \" America \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"(the[\\s]+|The[\\s]+)?United State(s)?\", \" America \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"\\(s\\)\", \" \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"[c-fC-F]\\:\\/\", \" disk \", text)\n",
    "    \n",
    "    # remove comma between numbers, i.e. 15,000 -> 15000\n",
    "    text = re.sub('(?<=[0-9])\\,(?=[0-9])', \"\", text)\n",
    "    \n",
    "    # add padding to punctuations and special chars, we still need them later\n",
    "    text = re.sub('\\$', \" dollar \", text)\n",
    "    text = re.sub('\\%', \" percent \", text)\n",
    "    text = re.sub('\\&', \" and \", text)\n",
    "    \n",
    "    text = re.sub('[^\\x00-\\x7F]+', pad_str(SPECIAL_TOKENS['non-ascii']), text) \n",
    "    \n",
    "    # indian dollar\n",
    "    text = re.sub(\"(?<=[0-9])rs \", \" rs \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\" rs(?=[0-9])\", \" rs \", text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # clean text rules get from : https://www.kaggle.com/currie32/the-importance-of-cleaning-text\n",
    "    text = re.sub(r\" (the[\\s]+|The[\\s]+)?US(A)? \", \" America \", text)\n",
    "    text = re.sub(r\" UK \", \" England \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" india \", \" India \", text)\n",
    "    text = re.sub(r\" switzerland \", \" Switzerland \", text)\n",
    "    text = re.sub(r\" china \", \" China \", text)\n",
    "    text = re.sub(r\" chinese \", \" Chinese \", text) \n",
    "    text = re.sub(r\" imrovement \", \" improvement \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" intially \", \" initially \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" quora \", \" Quora \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" dms \", \" direct messages \", text, flags=re.IGNORECASE)  \n",
    "    text = re.sub(r\" demonitization \", \" demonetization \", text, flags=re.IGNORECASE) \n",
    "    text = re.sub(r\" actived \", \" active \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" kms \", \" kilometers \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" cs \", \" computer science \", text, flags=re.IGNORECASE) \n",
    "    text = re.sub(r\" upvote\", \" up vote\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" iPhone \", \" phone \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" \\0rs \", \" rs \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" calender \", \" calendar \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" ios \", \" operating system \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" gps \", \" GPS \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" gst \", \" GST \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" programing \", \" programming \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" bestfriend \", \" best friend \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" dna \", \" DNA \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" III \", \" 3 \", text)\n",
    "    text = re.sub(r\" banglore \", \" Banglore \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" J K \", \" JK \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" J\\.K\\. \", \" JK \", text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # replace the float numbers with a random number, it will be parsed as number afterward, \n",
    "    # and also been replaced with word \"number\"\n",
    "    \n",
    "    text = re.sub('[0-9]+\\.[0-9]+', \" 87 \", text)\n",
    "  \n",
    "    \n",
    "    # Try 1. keeping and 2. removing puncutation to see if the results change.\n",
    "    # Result: Tried with random forest, there was no change in the result (except a slight change in confusion matrix).\n",
    "    # Remove punctuation from text.\n",
    "    text = ''.join([c for c in text if c not in punctuation]).lower()\n",
    "       \n",
    "    # Return a list of words\n",
    "    return text\n",
    "    \n",
    "df['question1'] = df['question1'].apply(clean)\n",
    "df['question2'] = df['question2'].apply(clean)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>what is the step by step guide to invest in sh...</td>\n",
       "      <td>what is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>what is the story of kohinoor kohinoor diamond</td>\n",
       "      <td>what would happen if the indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>how can i increase the speed of my internet co...</td>\n",
       "      <td>how can internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>why am i mentally very lonely how can i solve it</td>\n",
       "      <td>find the remainder when math2324math is divide...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>which one dissolve in water quikly sugar salt ...</td>\n",
       "      <td>which fish would survive in salt water</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>astrology i am a capricorn sun cap moon and ca...</td>\n",
       "      <td>i am a triple capricorn sun moon and ascendant...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>should i buy tiago</td>\n",
       "      <td>what keeps childern active and far from phone ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>how can i be a good geologist</td>\n",
       "      <td>what should i do to be a great geologist</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>when do you use  nonasciiword  instead of  non...</td>\n",
       "      <td>when do you use  and  instead of and</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>motorola company can i hack my charter motorol...</td>\n",
       "      <td>how do i hack motorola dcx3400 for free internet</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  what is the step by step guide to invest in sh...   \n",
       "1   1     3     4     what is the story of kohinoor kohinoor diamond   \n",
       "2   2     5     6  how can i increase the speed of my internet co...   \n",
       "3   3     7     8   why am i mentally very lonely how can i solve it   \n",
       "4   4     9    10  which one dissolve in water quikly sugar salt ...   \n",
       "5   5    11    12  astrology i am a capricorn sun cap moon and ca...   \n",
       "6   6    13    14                                 should i buy tiago   \n",
       "7   7    15    16                      how can i be a good geologist   \n",
       "8   8    17    18  when do you use  nonasciiword  instead of  non...   \n",
       "9   9    19    20  motorola company can i hack my charter motorol...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  what is the step by step guide to invest in sh...             0  \n",
       "1  what would happen if the indian government sto...             0  \n",
       "2  how can internet speed be increased by hacking...             0  \n",
       "3  find the remainder when math2324math is divide...             0  \n",
       "4             which fish would survive in salt water             0  \n",
       "5  i am a triple capricorn sun moon and ascendant...             1  \n",
       "6  what keeps childern active and far from phone ...             0  \n",
       "7           what should i do to be a great geologist             1  \n",
       "8               when do you use  and  instead of and             0  \n",
       "9   how do i hack motorola dcx3400 for free internet             0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at clean data\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is the step by step guide to invest in share market in india\n",
      "what is the step by step guide to invest in share market\n",
      "\n",
      "what is the story of kohinoor kohinoor diamond\n",
      "what would happen if the indian government stole the kohinoor kohinoor diamond back\n",
      "\n",
      "how can i increase the speed of my internet connection while using a vpn\n",
      "how can internet speed be increased by hacking through dns\n",
      "\n",
      "why am i mentally very lonely how can i solve it\n",
      "find the remainder when math2324math is divided by 2423\n",
      "\n",
      "which one dissolve in water quikly sugar salt methane and carbon di oxide\n",
      "which fish would survive in salt water\n",
      "\n",
      "astrology i am a capricorn sun cap moon and cap risingwhat does that say about me\n",
      "i am a triple capricorn sun moon and ascendant in capricorn what does this say about me\n",
      "\n",
      "should i buy tiago\n",
      "what keeps childern active and far from phone and video games\n",
      "\n",
      "how can i be a good geologist\n",
      "what should i do to be a great geologist\n",
      "\n",
      "when do you use  nonasciiword  instead of  nonasciiword \n",
      "when do you use  and  instead of and\n",
      "\n",
      "motorola company can i hack my charter motorolla dcx3400\n",
      "how do i hack motorola dcx3400 for free internet\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# After cleaning the text, we preview those question pairs again.\n",
    "a = 0 \n",
    "for i in range(a,a+10):\n",
    "    print(df.question1[i])\n",
    "    print(df.question2[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404287, 152570)\n",
      "(404287,)\n"
     ]
    }
   ],
   "source": [
    "# Use word level CountVectorizer to identify whether qestion1 is duplicate to question2 or not.\n",
    "\n",
    "# CountVectorizer documentation:\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "# Use word level CountVectorizer to identify whether qestion1 \n",
    "# is duplicate to question2 or not.\n",
    "\n",
    "# r'\\w{1,}' means 1 or more words\n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}') \n",
    "\n",
    "q1_trans = count_vect.fit_transform(df['question1'].values)\n",
    "q2_trans = count_vect.fit_transform(df['question2'].values)\n",
    "labels = df['is_duplicate'].values\n",
    "\n",
    "# Stack two questions using scipy sparse matrix, horizontally (column wise), \n",
    "# so as to make X variables as a 2-dimensional matrix, wehre\n",
    "# we can think of like we have two variables (features): q1_trans, and \n",
    "# q2_trans. Here we are not mixing two questions (q1_trans, and q2_trans) \n",
    "# to make one bag of words, but we are stacking them side by side \n",
    "# (still as two elements in a row of the matrix) to make X as a 2D matrix.\n",
    "\n",
    "X = scipy.sparse.hstack((q1_trans,q2_trans)) \n",
    "y = labels\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)), ('RF', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impuri...obs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False))])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# Using random forest classifier\n",
    "#\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# build a pipeline using RandomForestClassifier()\n",
    "rf_clf = Pipeline([('tfidf', TfidfTransformer()),('RF', RandomForestClassifier()),])\n",
    "\n",
    "# Feed the data through the pipeline\n",
    "rf_clf.fit(X_train, y_train)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      " [[70289  6320]\n",
      " [20682 23996]]\n",
      "\n",
      "Overall Accuracy:\n",
      "    77.74%\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.92      0.84     76609\n",
      "           1       0.79      0.54      0.64     44678\n",
      "\n",
      "   micro avg       0.78      0.78      0.78    121287\n",
      "   macro avg       0.78      0.73      0.74    121287\n",
      "weighted avg       0.78      0.78      0.77    121287\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Form a prediction set using random forest classifier\n",
    "predictionRF = rf_clf.predict(X_test)\n",
    "\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, predictionRF))\n",
    "print(\"\\nOverall Accuracy:\\n    {:.2f}%\".format(accuracy_score(y_test, predictionRF) * 100))\n",
    "print(\"\\nClassification report:\\n\", metrics.classification_report(y_test,predictionRF))\n",
    "\n",
    "# Confusion Matrix:\n",
    "#  [[70289  6320]\n",
    "#  [20682 23996]]\n",
    "\n",
    "# Overall Accuracy:\n",
    "#     77.74%\n",
    "\n",
    "# Classification report:\n",
    "#                precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.77      0.92      0.84     76609\n",
    "#            1       0.79      0.54      0.64     44678\n",
    "\n",
    "#    micro avg       0.78      0.78      0.78    121287\n",
    "#    macro avg       0.78      0.73      0.74    121287\n",
    "# weighted avg       0.78      0.78      0.77    121287\n",
    "\n",
    "# The accuracy here is 77.74% which is rather low but far better than the default value of 37%.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)), ('svc', LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0))])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# Build a pipeline using linear support vector classifier.\n",
    "#\n",
    "\n",
    "# build a pipeline using LinearSVC()\n",
    "# svc_clf = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),('svc', LinearSVC()),])\n",
    "# Do not use CountVectorizer as explained in here:\n",
    "# https://stackoverflow.com/questions/50192763/python-sklearn-pipiline-fit-attributeerror-lower-not-found?rq=1\n",
    "\"\"\"\n",
    "Either remove step ('vect', CountVectorizer()) or use TfidfTransformer instead of TfidfVectorizer \n",
    "as TfidfVectorizer expects array of strings as an input and CountVectorizer() returns a matrix of \n",
    "occurances (i.e. numeric matrix).\n",
    "\n",
    "Per default TfidfVectorizer(..., lowercase=True) will try to \"lowercase\" all strings, hence \n",
    "the “AttributeError: lower not found” error message.\n",
    "\"\"\"\n",
    "# build a pipeline using LinearSVC()\n",
    "svc_clf = Pipeline([('tfidf', TfidfTransformer()),('svc', LinearSVC()),])\n",
    "\n",
    "# Feed the data through the pipeline\n",
    "svc_clf.fit(X_train, y_train)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      " [[64560 12049]\n",
      " [17106 27572]]\n",
      "\n",
      "Overall Accuracy:\n",
      "    75.96%\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.84      0.82     76609\n",
      "           1       0.70      0.62      0.65     44678\n",
      "\n",
      "   micro avg       0.76      0.76      0.76    121287\n",
      "   macro avg       0.74      0.73      0.73    121287\n",
      "weighted avg       0.76      0.76      0.76    121287\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Form a prediction set using linear support vector classifier.\n",
    "predictionSV = svc_clf.predict(X_test)\n",
    "\n",
    "# Get the score report\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, predictionSV))\n",
    "print(\"\\nOverall Accuracy:\\n    {:.2f}%\".format(accuracy_score(y_test, predictionSV) * 100))\n",
    "print(\"\\nClassification report:\\n\", metrics.classification_report(y_test,predictionSV))\n",
    "\n",
    "\n",
    "# Confusion Matrix:\n",
    "#  [[64560 12049]\n",
    "#  [17106 27572]]\n",
    "\n",
    "# Overall Accuracy:\n",
    "#     75.96%\n",
    "\n",
    "# Classification report:\n",
    "#                precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.79      0.84      0.82     76609\n",
    "#            1       0.70      0.62      0.65     44678\n",
    "\n",
    "#    micro avg       0.76      0.76      0.76    121287\n",
    "#    macro avg       0.74      0.73      0.73    121287\n",
    "# weighted avg       0.76      0.76      0.76    121287\n",
    "\n",
    "\n",
    "# The accuracy we got is 75.96% with this method. We try another method next to improve it if possible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Character Level TF-IDF to identify whether qestion1 is duplicate to question2 or not.\n",
    "\n",
    "# Note that TF-IDF based method does not perform well when there is no vocabulary overlap \n",
    "# but there is semantic similarity between sentences. \n",
    "# So we must use word2vec (toward the end section of this project). \n",
    "\n",
    "# r'\\w{1,}' means 1 or more words\n",
    "# n-gram means sequence of n words. \n",
    "# ngram_range=(2,3) means we use bigrams or trigrams.\n",
    "# max_features=5000 means we use 5000 columns at maximum.\n",
    "\n",
    "tfidf_vect_ngram_chars=TfidfVectorizer(analyzer='char',token_pattern=r'\\w{1,}',ngram_range=(2,3),max_features=5000)\n",
    "\n",
    "q1_trans = tfidf_vect_ngram_chars.fit_transform(df['question1'].values)\n",
    "q2_trans = tfidf_vect_ngram_chars.fit_transform(df['question2'].values)\n",
    "labels = df['is_duplicate'].values\n",
    "\n",
    "# Stack two questions, using sparse matrix, horizontally (column wise) \n",
    "# so as to make X variable a 2-dimensional matrix, wehre\n",
    "# we can think of like we have two variables (features): q1_trans, and q2_trans.\n",
    "# Here we are not mixing two questions (q1_trans, and q2_trans) to make one bag of words, \n",
    "# but we are stacking them side by side (still as two elements in a row of the matrix) to make X as a 2D matrix.\n",
    "X = scipy.sparse.hstack((q1_trans,q2_trans))\n",
    "y = labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404287, 5000)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1_trans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)), ('svc', LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0))])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# build a pipeline using LinearSVC()\n",
    "#\n",
    "\n",
    "svc_clfChar = Pipeline([('tfidf', TfidfTransformer()),('svc', LinearSVC()),])\n",
    "\n",
    "# Feed the data through the pipeline\n",
    "svc_clfChar.fit(X_train, y_train)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      " [[65707 10902]\n",
      " [20255 24423]]\n",
      "\n",
      "Overall Accuracy:\n",
      "    74.31%\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.86      0.81     76609\n",
      "           1       0.69      0.55      0.61     44678\n",
      "\n",
      "   micro avg       0.74      0.74      0.74    121287\n",
      "   macro avg       0.73      0.70      0.71    121287\n",
      "weighted avg       0.74      0.74      0.74    121287\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Form a prediction set\n",
    "predictionSVchar = svc_clfChar.predict(X_test)\n",
    "\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, predictionSVchar))\n",
    "print(\"\\nOverall Accuracy:\\n    {:.2f}%\".format(accuracy_score(y_test, predictionSVchar) * 100))\n",
    "print(\"\\nClassification report:\\n\", metrics.classification_report(y_test,predictionSVchar))\n",
    "\n",
    "\n",
    "# Confusion Matrix:\n",
    "#  [[65707 10902]\n",
    "#  [20255 24423]]\n",
    "\n",
    "# Overall Accuracy:\n",
    "#     74.31%\n",
    "\n",
    "# Classification report:\n",
    "#                precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.76      0.86      0.81     76609\n",
    "#            1       0.69      0.55      0.61     44678\n",
    "\n",
    "#    micro avg       0.74      0.74      0.74    121287\n",
    "#    macro avg       0.73      0.70      0.71    121287\n",
    "# weighted avg       0.74      0.74      0.74    121287\n",
    "\n",
    "# The accuracy we got now went down from 75% to 74% with this method - not good.\n",
    "# We add Xgboost next to improve accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "character level tf-idf training score: 0.997985723769572\n",
      "character level tf-idf test score: 0.8048011235004698\n",
      "\n",
      "Confusion Matrix:\n",
      " [[69878  6731]\n",
      " [14418 30260]]\n",
      "\n",
      "Overall Accuracy:\n",
      "    82.56%\n",
      "\n",
      "classification_report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.91      0.87     76609\n",
      "           1       0.82      0.68      0.74     44678\n",
      "\n",
      "   micro avg       0.83      0.83      0.83    121287\n",
      "   macro avg       0.82      0.79      0.80    121287\n",
      "weighted avg       0.82      0.83      0.82    121287\n",
      "\n",
      "Code run-time:  2:36:07.556734\n"
     ]
    }
   ],
   "source": [
    "# Use Character Level TF-IDF + Xgboost to identify whether qestion1 is duplicate to question2 or not.\n",
    "\n",
    "# Character Level TF-IDF + Xgboost (Extreme Gradient Boosting)\n",
    "\n",
    "# XGBoost (Extreme Gradient Boosting):  https://www.datacamp.com/community/tutorials/xgboost-in-python\n",
    "# XGBoost is an implementation of gradient boosted decision trees designed for speed \n",
    "# and performance that is dominative competitive machine learning. It has shown better performance \n",
    "# on a variety of machine learning benchmark datasets. XGBoost internally has parameters for \n",
    "# cross-validation, regularization, user-defined objective functions, missing values, \n",
    "# tree parameters, scikit-learn compatible API etc.\n",
    "\n",
    "# What is boosting? https://www.datacamp.com/community/tutorials/xgboost-in-python\n",
    "# Boosting is a sequential technique which works on the principle of an ensemble. \n",
    "# It combines a set of weak learners and delivers improved prediction accuracy. At any instant t, \n",
    "# the model outcomes are weighed based on the outcomes of previous instant t-1. The outcomes predicted \n",
    "# correctly are given a lower weight and the ones miss-classified are weighted higher. Note that a \n",
    "# weak learner is one which is slightly better than random guessing. For example, a decision tree whose \n",
    "# predictions are slightly better than 50%.\n",
    "\n",
    "\n",
    "# TfidfVectorizer -- Brief Tutorial:  tf: term frequency, idf: inverse document frequency\n",
    "#https://www.kaggle.com/adamschroeder/countvectorizer-tfidfvectorizer-predict-comments\n",
    "\n",
    "# The goal of using tf-idf is to scale down the impact of tokens that occur very frequently \n",
    "# in a given corpus and that are hence empirically less informative than features that occur \n",
    "# in a small fraction of the training corpus. We want low positive weights for frequent terms \n",
    "# and high weights for rare terms.\n",
    "\n",
    "# TF: How often does the term appear in a document? The more often, the higher the weight.\n",
    "# IDF: How often does the term appear in all documents in the collection? The more often, the lower the weight.\n",
    "# The product TF * IDF will scale down the impact of tokens that occur very frequently, but are\n",
    "# less informative, in the document.\n",
    "\n",
    "# TF-IDF Source - see last example here: https://en.wikipedia.org/wiki/Tf–idf \n",
    "\n",
    "import datetime\n",
    "st = datetime.datetime.now() # current time\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(max_depth=50, n_estimators=80, learning_rate=0.1, colsample_bytree=.7, \n",
    "                              gamma=0, reg_alpha=4, objective='binary:logistic', eta=0.3, silent=1, \n",
    "                              subsample=0.8).fit(X_train, y_train) \n",
    "xgb_prediction = xgb_model.predict(X_test)\n",
    "\n",
    "print('character level tf-idf training score:', f1_score(y_train, xgb_model.predict(X_train), average='macro'))\n",
    "print('character level tf-idf test score:', f1_score(y_test, xgb_model.predict(X_test), average='macro'))\n",
    "\n",
    "\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, xgb_prediction))\n",
    "print(\"\\nOverall Accuracy:\\n    {:.2f}%\".format(accuracy_score(y_test, xgb_prediction) * 100))\n",
    "print(\"\\nclassification_report\\n\", classification_report(y_test, xgb_prediction))\n",
    "\n",
    "nd = datetime.datetime.now() # end time now\n",
    "print(\"Code run-time: \", nd-st) # it's the total time to run the code (finish time -start time)\n",
    "\n",
    "\n",
    "# character level tf-idf training score: 0.997985723769572\n",
    "# character level tf-idf test score: 0.8048011235004698\n",
    "\n",
    "# Confusion Matrix:\n",
    "#  [[69878  6731]\n",
    "#  [14418 30260]]\n",
    "\n",
    "# Overall Accuracy:\n",
    "#     82.56%\n",
    "\n",
    "# classification_report\n",
    "#                precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.83      0.91      0.87     76609\n",
    "#            1       0.82      0.68      0.74     44678\n",
    "\n",
    "#    micro avg       0.83      0.83      0.83    121287\n",
    "#    macro avg       0.82      0.79      0.80    121287\n",
    "# weighted avg       0.82      0.83      0.82    121287\n",
    "\n",
    "# Code run-time:  2:36:07.556734\n",
    "\n",
    "\n",
    "# Now the accuracy is 82.56% which is the best we have.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# All code below this runs independently of any code written above.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#\n",
    "# To capture sentiment, we use MaLSTM neural net.\n",
    "#\n",
    "\n",
    "import re\n",
    "import gensim\n",
    "import logging\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import KeyedVectors\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.layers import Layer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "\n",
    "# Clean up the text\n",
    "def text_to_word_list(text):\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = text.split()\n",
    "\n",
    "    return text\n",
    "\n",
    "# We use the 300-dimensional word2vec embeddings from google which can capture intricate\n",
    "# inter-word relationships such as vec(king) − vec(man) + vec(woman) ≈ vec(queen).\n",
    "# This is publicly available at: code.google.com/p/word2vec\n",
    "def make_w2v_embeddings(df, embedding_dim=300):\n",
    "    vocabs = {}\n",
    "    vocabs_cnt = 0\n",
    "    vocabs_not_w2v = {}\n",
    "    vocabs_not_w2v_cnt = 0\n",
    "\n",
    "    stops = set(stopwords.words('english'))\n",
    "\n",
    "    word2vec = KeyedVectors.load_word2vec_format(\"/Users/rameshsubedi/Downloads/nlpDownload/GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if index != 0 and index % 1000 == 0:\n",
    "            print(\"{:,} sentences embedded.\".format(index), flush=True)\n",
    "\n",
    "        for question in ['question1', 'question2']:\n",
    "\n",
    "            q2n = []\n",
    "            for word in text_to_word_list(row[question]):\n",
    "                if word in stops:\n",
    "                    continue\n",
    "                if word not in word2vec.vocab:\n",
    "                    if word not in vocabs_not_w2v:\n",
    "                        vocabs_not_w2v_cnt += 1\n",
    "                        vocabs_not_w2v[word] = 1\n",
    "                if word not in vocabs:\n",
    "                    vocabs_cnt += 1\n",
    "                    vocabs[word] = vocabs_cnt\n",
    "                    q2n.append(vocabs_cnt)\n",
    "                else:\n",
    "                    q2n.append(vocabs[word])\n",
    "\n",
    "            df.at[index, question + '_n'] = q2n\n",
    "\n",
    "    embeddings = 1 * np.random.randn(len(vocabs) + 1, embedding_dim)\n",
    "    embeddings[0] = 0\n",
    "\n",
    "    for word, index in vocabs.items():\n",
    "        if word in word2vec.vocab:\n",
    "            embeddings[index] = word2vec.word_vec(word)\n",
    "    del word2vec\n",
    "\n",
    "    return df, embeddings\n",
    "\n",
    "\n",
    "def split_and_zero_padding(df, max_seq_length):\n",
    "    X = {'q1': df['question1_n'], 'q2': df['question2_n']}\n",
    "\n",
    "    for dataset, side in itertools.product([X], ['q1', 'q2']):\n",
    "        dataset[side] = pad_sequences(dataset[side], padding='pre', truncating='post', maxlen=max_seq_length)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "class ManhattanDistance(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ManhattanDistance, self).__init__(**kwargs)\n",
    "        self.result = None\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(ManhattanDistance, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, **kwargs):\n",
    "        self.result = K.exp(-K.sum(K.abs(x[0] - x[1]), axis=1, keepdims=True))\n",
    "        return self.result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return K.int_shape(self.result)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import matplotlib\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from time import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.models import Model, Sequential\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.python.keras.layers import Input, Embedding, LSTM\n",
    "\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "#ROOT_PATH = os.path.abspath('/Users/rameshsubedi/Downloads/kaggle')\n",
    "ROOT_PATH = os.path.abspath('') # It's /Users/rameshsubedi\n",
    "TRAIN_CSV = 'train.csv'\n",
    "\n",
    "gpus = 2\n",
    "#batch_size = 256 * gpus\n",
    "batch_size = 128 * gpus\n",
    "\n",
    "n_epoch = 50 # we will be training the whole data (the training data part) 50 times.\n",
    "#  To learn more about epoch, batch, look here: \n",
    "# https://stackoverflow.com/questions/4752626/epoch-vs-iteration-when-training-neural-networks\n",
    "\"\"\"\n",
    "An EPOCH describes the number of times the algorithm sees the entire data set. \n",
    "So, each time the algorithm has seen all samples in the dataset, an epoch has completed.\n",
    "\n",
    "An ITERATION describes the number of times a BATCH of data passed through the algorithm. \n",
    "In the case of neural networks, that means (or ITERATION means) the forward pass and backward pass. \n",
    "So, every time you pass a batch of data through the NN, you completed an iteration.\n",
    "\n",
    "Example: if you have 1000 training examples (rows), and your batch size is 500, \n",
    "then it will take 2 iterations to complete 1 epoch.\n",
    "\n",
    "In this analysis there are 404287 rows, and batch size is 256, so it will take 404287/256=1579.246\n",
    "iterations complete one epoch.\n",
    "\n",
    "\"\"\"\n",
    "#n_hidden = 256\n",
    "n_hidden = 128\n",
    "\n",
    "# using 300 dimension for embedding i.e. there will be 300 vectors for each word in the corpora \n",
    "# represented for neural network model.\n",
    "embedding_dim = 300\n",
    "max_seq_length = 25  # length of the longest question has 25 words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_data():\n",
    "    train_df = pd.read_csv(os.path.join(ROOT_PATH, TRAIN_CSV))\n",
    "    for q in ['question1', 'question2']:\n",
    "        train_df[q + '_n'] = train_df[q]\n",
    "\n",
    "    train_df, embeddings = make_w2v_embeddings(train_df, embedding_dim=embedding_dim)\n",
    "    #validation_size = int(len(train_df) * 0.1)\n",
    "    validation_size = int(len(train_df) * 0.3) # 30% for test (here validation means test), 70% for train\n",
    "\n",
    "    X = train_df[['question1_n', 'question2_n']]\n",
    "    Y = train_df['is_duplicate']\n",
    "\n",
    "    #X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=validation_size)\n",
    "    X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=validation_size, random_state=42)\n",
    "\n",
    "\n",
    "    X_train = split_and_zero_padding(X_train, max_seq_length)\n",
    "    X_validation = split_and_zero_padding(X_validation, max_seq_length)\n",
    "\n",
    "    Y_train = Y_train.values\n",
    "    Y_validation = Y_validation.values\n",
    "\n",
    "    assert X_train['q1'].shape == X_train['q2'].shape\n",
    "    assert len(X_train['q1']) == len(Y_train)\n",
    "\n",
    "    return X_train, X_validation, Y_train, Y_validation, embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_model(embeddings):\n",
    "    shared_model = Sequential() \n",
    "    # Set trainble=False so that the gradient decent will not optimize the embeddings.\n",
    "    shared_model.add(Embedding(len(embeddings), embedding_dim, weights=[embeddings], input_shape=(max_seq_length,), trainable=False))\n",
    "    shared_model.add(LSTM(n_hidden))\n",
    "\n",
    "    q1_input = Input(shape=(max_seq_length,), dtype='int32')\n",
    "    q2_input = Input(shape=(max_seq_length,), dtype='int32')\n",
    "\n",
    "    malstm_distance = ManhattanDistance()([shared_model(q1_input), shared_model(q2_input)])\n",
    "    \n",
    "    # The Model below is due from:  from tensorflow.python.keras.models import Model\n",
    "    model = Model(inputs=[q1_input, q2_input], outputs=[malstm_distance]) \n",
    "\n",
    "    # if gpus >= 2:\n",
    "    #     model = tf.keras.utils.multi_gpu_model(model, gpus=gpus)\n",
    "   \n",
    "   # Adam() from here:   from tensorflow.python.keras.optimizers import Adam\n",
    "    model.compile(loss='mean_squared_error', optimizer=Adam(), metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    shared_model.summary()\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(X_train, X_validation, Y_train, Y_validation, model):\n",
    "    training_start_time = time()\n",
    "\n",
    "    checkpointer = ModelCheckpoint(\n",
    "        filepath=os.path.join(ROOT_PATH, 'model/weights.{epoch:02d}.h5'),\n",
    "        verbose=1, save_best_only=True, save_weights_only=False, monitor='val_loss', mode='min', period=1)\n",
    "        \n",
    "    malstm_trained = model.fit(\n",
    "        [X_train['q1'], X_train['q2']], Y_train, batch_size=batch_size, epochs=n_epoch,\n",
    "        validation_data=([X_validation['q1'], X_validation['q2']], Y_validation), \n",
    "        callbacks=[checkpointer])\n",
    "    \n",
    "    training_end_time = time()\n",
    "    print(\"Training time finished.\\n%d epochs in %12.2f\" % (n_epoch, training_end_time - training_start_time))\n",
    "\n",
    "    model.save(os.path.join(ROOT_PATH, 'model/siamese-lstm-weights.h5')) # This file will be created when we run the main function below.\n",
    "\n",
    "    return malstm_trained\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy_and_loss(malstm_trained):\n",
    "    plt.subplot(211)\n",
    "    plt.plot(malstm_trained.history['acc'])\n",
    "    plt.plot(malstm_trained.history['val_acc'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left') \n",
    "\n",
    "    plt.subplot(212)\n",
    "    plt.plot(malstm_trained.history['loss'])\n",
    "    plt.plot(malstm_trained.history['val_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper right') \n",
    "\n",
    "    plt.tight_layout(h_pad=1.0)\n",
    "    plt.savefig(os.path.join(ROOT_PATH, 'model/history-graph.png')) # This graph will be made when we run the main function below.\n",
    "    print(str(malstm_trained.history['val_acc'][-1])[:6] + \"(max: \" + str(max(malstm_trained.history['val_acc']))[:6] + \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-17 23:58:41,638 : INFO : loading projection weights from /Users/rameshsubedi/Downloads/nlpDownload/GoogleNews-vectors-negative300.bin\n",
      "2019-04-18 00:00:55,201 : INFO : loaded (3000000, 300) matrix from /Users/rameshsubedi/Downloads/nlpDownload/GoogleNews-vectors-negative300.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,000 sentences embedded.\n",
      "2,000 sentences embedded.\n",
      "3,000 sentences embedded.\n",
      "4,000 sentences embedded.\n",
      "5,000 sentences embedded.\n",
      "6,000 sentences embedded.\n",
      "7,000 sentences embedded.\n",
      "8,000 sentences embedded.\n",
      "9,000 sentences embedded.\n",
      "10,000 sentences embedded.\n",
      "11,000 sentences embedded.\n",
      "12,000 sentences embedded.\n",
      "13,000 sentences embedded.\n",
      "14,000 sentences embedded.\n",
      "15,000 sentences embedded.\n",
      "16,000 sentences embedded.\n",
      "17,000 sentences embedded.\n",
      "18,000 sentences embedded.\n",
      "19,000 sentences embedded.\n",
      "20,000 sentences embedded.\n",
      "21,000 sentences embedded.\n",
      "22,000 sentences embedded.\n",
      "23,000 sentences embedded.\n",
      "24,000 sentences embedded.\n",
      "25,000 sentences embedded.\n",
      "26,000 sentences embedded.\n",
      "27,000 sentences embedded.\n",
      "28,000 sentences embedded.\n",
      "29,000 sentences embedded.\n",
      "30,000 sentences embedded.\n",
      "31,000 sentences embedded.\n",
      "32,000 sentences embedded.\n",
      "33,000 sentences embedded.\n",
      "34,000 sentences embedded.\n",
      "35,000 sentences embedded.\n",
      "36,000 sentences embedded.\n",
      "37,000 sentences embedded.\n",
      "38,000 sentences embedded.\n",
      "39,000 sentences embedded.\n",
      "40,000 sentences embedded.\n",
      "41,000 sentences embedded.\n",
      "42,000 sentences embedded.\n",
      "43,000 sentences embedded.\n",
      "44,000 sentences embedded.\n",
      "45,000 sentences embedded.\n",
      "46,000 sentences embedded.\n",
      "47,000 sentences embedded.\n",
      "48,000 sentences embedded.\n",
      "49,000 sentences embedded.\n",
      "50,000 sentences embedded.\n",
      "51,000 sentences embedded.\n",
      "52,000 sentences embedded.\n",
      "53,000 sentences embedded.\n",
      "54,000 sentences embedded.\n",
      "55,000 sentences embedded.\n",
      "56,000 sentences embedded.\n",
      "57,000 sentences embedded.\n",
      "58,000 sentences embedded.\n",
      "59,000 sentences embedded.\n",
      "60,000 sentences embedded.\n",
      "61,000 sentences embedded.\n",
      "62,000 sentences embedded.\n",
      "63,000 sentences embedded.\n",
      "64,000 sentences embedded.\n",
      "65,000 sentences embedded.\n",
      "66,000 sentences embedded.\n",
      "67,000 sentences embedded.\n",
      "68,000 sentences embedded.\n",
      "69,000 sentences embedded.\n",
      "70,000 sentences embedded.\n",
      "71,000 sentences embedded.\n",
      "72,000 sentences embedded.\n",
      "73,000 sentences embedded.\n",
      "74,000 sentences embedded.\n",
      "75,000 sentences embedded.\n",
      "76,000 sentences embedded.\n",
      "77,000 sentences embedded.\n",
      "78,000 sentences embedded.\n",
      "79,000 sentences embedded.\n",
      "80,000 sentences embedded.\n",
      "81,000 sentences embedded.\n",
      "82,000 sentences embedded.\n",
      "83,000 sentences embedded.\n",
      "84,000 sentences embedded.\n",
      "85,000 sentences embedded.\n",
      "86,000 sentences embedded.\n",
      "87,000 sentences embedded.\n",
      "88,000 sentences embedded.\n",
      "89,000 sentences embedded.\n",
      "90,000 sentences embedded.\n",
      "91,000 sentences embedded.\n",
      "92,000 sentences embedded.\n",
      "93,000 sentences embedded.\n",
      "94,000 sentences embedded.\n",
      "95,000 sentences embedded.\n",
      "96,000 sentences embedded.\n",
      "97,000 sentences embedded.\n",
      "98,000 sentences embedded.\n",
      "99,000 sentences embedded.\n",
      "100,000 sentences embedded.\n",
      "101,000 sentences embedded.\n",
      "102,000 sentences embedded.\n",
      "103,000 sentences embedded.\n",
      "104,000 sentences embedded.\n",
      "105,000 sentences embedded.\n",
      "106,000 sentences embedded.\n",
      "107,000 sentences embedded.\n",
      "108,000 sentences embedded.\n",
      "109,000 sentences embedded.\n",
      "110,000 sentences embedded.\n",
      "111,000 sentences embedded.\n",
      "112,000 sentences embedded.\n",
      "113,000 sentences embedded.\n",
      "114,000 sentences embedded.\n",
      "115,000 sentences embedded.\n",
      "116,000 sentences embedded.\n",
      "117,000 sentences embedded.\n",
      "118,000 sentences embedded.\n",
      "119,000 sentences embedded.\n",
      "120,000 sentences embedded.\n",
      "121,000 sentences embedded.\n",
      "122,000 sentences embedded.\n",
      "123,000 sentences embedded.\n",
      "124,000 sentences embedded.\n",
      "125,000 sentences embedded.\n",
      "126,000 sentences embedded.\n",
      "127,000 sentences embedded.\n",
      "128,000 sentences embedded.\n",
      "129,000 sentences embedded.\n",
      "130,000 sentences embedded.\n",
      "131,000 sentences embedded.\n",
      "132,000 sentences embedded.\n",
      "133,000 sentences embedded.\n",
      "134,000 sentences embedded.\n",
      "135,000 sentences embedded.\n",
      "136,000 sentences embedded.\n",
      "137,000 sentences embedded.\n",
      "138,000 sentences embedded.\n",
      "139,000 sentences embedded.\n",
      "140,000 sentences embedded.\n",
      "141,000 sentences embedded.\n",
      "142,000 sentences embedded.\n",
      "143,000 sentences embedded.\n",
      "144,000 sentences embedded.\n",
      "145,000 sentences embedded.\n",
      "146,000 sentences embedded.\n",
      "147,000 sentences embedded.\n",
      "148,000 sentences embedded.\n",
      "149,000 sentences embedded.\n",
      "150,000 sentences embedded.\n",
      "151,000 sentences embedded.\n",
      "152,000 sentences embedded.\n",
      "153,000 sentences embedded.\n",
      "154,000 sentences embedded.\n",
      "155,000 sentences embedded.\n",
      "156,000 sentences embedded.\n",
      "157,000 sentences embedded.\n",
      "158,000 sentences embedded.\n",
      "159,000 sentences embedded.\n",
      "160,000 sentences embedded.\n",
      "161,000 sentences embedded.\n",
      "162,000 sentences embedded.\n",
      "163,000 sentences embedded.\n",
      "164,000 sentences embedded.\n",
      "165,000 sentences embedded.\n",
      "166,000 sentences embedded.\n",
      "167,000 sentences embedded.\n",
      "168,000 sentences embedded.\n",
      "169,000 sentences embedded.\n",
      "170,000 sentences embedded.\n",
      "171,000 sentences embedded.\n",
      "172,000 sentences embedded.\n",
      "173,000 sentences embedded.\n",
      "174,000 sentences embedded.\n",
      "175,000 sentences embedded.\n",
      "176,000 sentences embedded.\n",
      "177,000 sentences embedded.\n",
      "178,000 sentences embedded.\n",
      "179,000 sentences embedded.\n",
      "180,000 sentences embedded.\n",
      "181,000 sentences embedded.\n",
      "182,000 sentences embedded.\n",
      "183,000 sentences embedded.\n",
      "184,000 sentences embedded.\n",
      "185,000 sentences embedded.\n",
      "186,000 sentences embedded.\n",
      "187,000 sentences embedded.\n",
      "188,000 sentences embedded.\n",
      "189,000 sentences embedded.\n",
      "190,000 sentences embedded.\n",
      "191,000 sentences embedded.\n",
      "192,000 sentences embedded.\n",
      "193,000 sentences embedded.\n",
      "194,000 sentences embedded.\n",
      "195,000 sentences embedded.\n",
      "196,000 sentences embedded.\n",
      "197,000 sentences embedded.\n",
      "198,000 sentences embedded.\n",
      "199,000 sentences embedded.\n",
      "200,000 sentences embedded.\n",
      "201,000 sentences embedded.\n",
      "202,000 sentences embedded.\n",
      "203,000 sentences embedded.\n",
      "204,000 sentences embedded.\n",
      "205,000 sentences embedded.\n",
      "206,000 sentences embedded.\n",
      "207,000 sentences embedded.\n",
      "208,000 sentences embedded.\n",
      "209,000 sentences embedded.\n",
      "210,000 sentences embedded.\n",
      "211,000 sentences embedded.\n",
      "212,000 sentences embedded.\n",
      "213,000 sentences embedded.\n",
      "214,000 sentences embedded.\n",
      "215,000 sentences embedded.\n",
      "216,000 sentences embedded.\n",
      "217,000 sentences embedded.\n",
      "218,000 sentences embedded.\n",
      "219,000 sentences embedded.\n",
      "220,000 sentences embedded.\n",
      "221,000 sentences embedded.\n",
      "222,000 sentences embedded.\n",
      "223,000 sentences embedded.\n",
      "224,000 sentences embedded.\n",
      "225,000 sentences embedded.\n",
      "226,000 sentences embedded.\n",
      "227,000 sentences embedded.\n",
      "228,000 sentences embedded.\n",
      "229,000 sentences embedded.\n",
      "230,000 sentences embedded.\n",
      "231,000 sentences embedded.\n",
      "232,000 sentences embedded.\n",
      "233,000 sentences embedded.\n",
      "234,000 sentences embedded.\n",
      "235,000 sentences embedded.\n",
      "236,000 sentences embedded.\n",
      "237,000 sentences embedded.\n",
      "238,000 sentences embedded.\n",
      "239,000 sentences embedded.\n",
      "240,000 sentences embedded.\n",
      "241,000 sentences embedded.\n",
      "242,000 sentences embedded.\n",
      "243,000 sentences embedded.\n",
      "244,000 sentences embedded.\n",
      "245,000 sentences embedded.\n",
      "246,000 sentences embedded.\n",
      "247,000 sentences embedded.\n",
      "248,000 sentences embedded.\n",
      "249,000 sentences embedded.\n",
      "250,000 sentences embedded.\n",
      "251,000 sentences embedded.\n",
      "252,000 sentences embedded.\n",
      "253,000 sentences embedded.\n",
      "254,000 sentences embedded.\n",
      "255,000 sentences embedded.\n",
      "256,000 sentences embedded.\n",
      "257,000 sentences embedded.\n",
      "258,000 sentences embedded.\n",
      "259,000 sentences embedded.\n",
      "260,000 sentences embedded.\n",
      "261,000 sentences embedded.\n",
      "262,000 sentences embedded.\n",
      "263,000 sentences embedded.\n",
      "264,000 sentences embedded.\n",
      "265,000 sentences embedded.\n",
      "266,000 sentences embedded.\n",
      "267,000 sentences embedded.\n",
      "268,000 sentences embedded.\n",
      "269,000 sentences embedded.\n",
      "270,000 sentences embedded.\n",
      "271,000 sentences embedded.\n",
      "272,000 sentences embedded.\n",
      "273,000 sentences embedded.\n",
      "274,000 sentences embedded.\n",
      "275,000 sentences embedded.\n",
      "276,000 sentences embedded.\n",
      "277,000 sentences embedded.\n",
      "278,000 sentences embedded.\n",
      "279,000 sentences embedded.\n",
      "280,000 sentences embedded.\n",
      "281,000 sentences embedded.\n",
      "282,000 sentences embedded.\n",
      "283,000 sentences embedded.\n",
      "284,000 sentences embedded.\n",
      "285,000 sentences embedded.\n",
      "286,000 sentences embedded.\n",
      "287,000 sentences embedded.\n",
      "288,000 sentences embedded.\n",
      "289,000 sentences embedded.\n",
      "290,000 sentences embedded.\n",
      "291,000 sentences embedded.\n",
      "292,000 sentences embedded.\n",
      "293,000 sentences embedded.\n",
      "294,000 sentences embedded.\n",
      "295,000 sentences embedded.\n",
      "296,000 sentences embedded.\n",
      "297,000 sentences embedded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "298,000 sentences embedded.\n",
      "299,000 sentences embedded.\n",
      "300,000 sentences embedded.\n",
      "301,000 sentences embedded.\n",
      "302,000 sentences embedded.\n",
      "303,000 sentences embedded.\n",
      "304,000 sentences embedded.\n",
      "305,000 sentences embedded.\n",
      "306,000 sentences embedded.\n",
      "307,000 sentences embedded.\n",
      "308,000 sentences embedded.\n",
      "309,000 sentences embedded.\n",
      "310,000 sentences embedded.\n",
      "311,000 sentences embedded.\n",
      "312,000 sentences embedded.\n",
      "313,000 sentences embedded.\n",
      "314,000 sentences embedded.\n",
      "315,000 sentences embedded.\n",
      "316,000 sentences embedded.\n",
      "317,000 sentences embedded.\n",
      "318,000 sentences embedded.\n",
      "319,000 sentences embedded.\n",
      "320,000 sentences embedded.\n",
      "321,000 sentences embedded.\n",
      "322,000 sentences embedded.\n",
      "323,000 sentences embedded.\n",
      "324,000 sentences embedded.\n",
      "325,000 sentences embedded.\n",
      "326,000 sentences embedded.\n",
      "327,000 sentences embedded.\n",
      "328,000 sentences embedded.\n",
      "329,000 sentences embedded.\n",
      "330,000 sentences embedded.\n",
      "331,000 sentences embedded.\n",
      "332,000 sentences embedded.\n",
      "333,000 sentences embedded.\n",
      "334,000 sentences embedded.\n",
      "335,000 sentences embedded.\n",
      "336,000 sentences embedded.\n",
      "337,000 sentences embedded.\n",
      "338,000 sentences embedded.\n",
      "339,000 sentences embedded.\n",
      "340,000 sentences embedded.\n",
      "341,000 sentences embedded.\n",
      "342,000 sentences embedded.\n",
      "343,000 sentences embedded.\n",
      "344,000 sentences embedded.\n",
      "345,000 sentences embedded.\n",
      "346,000 sentences embedded.\n",
      "347,000 sentences embedded.\n",
      "348,000 sentences embedded.\n",
      "349,000 sentences embedded.\n",
      "350,000 sentences embedded.\n",
      "351,000 sentences embedded.\n",
      "352,000 sentences embedded.\n",
      "353,000 sentences embedded.\n",
      "354,000 sentences embedded.\n",
      "355,000 sentences embedded.\n",
      "356,000 sentences embedded.\n",
      "357,000 sentences embedded.\n",
      "358,000 sentences embedded.\n",
      "359,000 sentences embedded.\n",
      "360,000 sentences embedded.\n",
      "361,000 sentences embedded.\n",
      "362,000 sentences embedded.\n",
      "363,000 sentences embedded.\n",
      "364,000 sentences embedded.\n",
      "365,000 sentences embedded.\n",
      "366,000 sentences embedded.\n",
      "367,000 sentences embedded.\n",
      "368,000 sentences embedded.\n",
      "369,000 sentences embedded.\n",
      "370,000 sentences embedded.\n",
      "371,000 sentences embedded.\n",
      "372,000 sentences embedded.\n",
      "373,000 sentences embedded.\n",
      "374,000 sentences embedded.\n",
      "375,000 sentences embedded.\n",
      "376,000 sentences embedded.\n",
      "377,000 sentences embedded.\n",
      "378,000 sentences embedded.\n",
      "379,000 sentences embedded.\n",
      "380,000 sentences embedded.\n",
      "381,000 sentences embedded.\n",
      "382,000 sentences embedded.\n",
      "383,000 sentences embedded.\n",
      "384,000 sentences embedded.\n",
      "385,000 sentences embedded.\n",
      "386,000 sentences embedded.\n",
      "387,000 sentences embedded.\n",
      "388,000 sentences embedded.\n",
      "389,000 sentences embedded.\n",
      "390,000 sentences embedded.\n",
      "391,000 sentences embedded.\n",
      "392,000 sentences embedded.\n",
      "393,000 sentences embedded.\n",
      "394,000 sentences embedded.\n",
      "395,000 sentences embedded.\n",
      "396,000 sentences embedded.\n",
      "397,000 sentences embedded.\n",
      "398,000 sentences embedded.\n",
      "399,000 sentences embedded.\n",
      "400,000 sentences embedded.\n",
      "401,000 sentences embedded.\n",
      "402,000 sentences embedded.\n",
      "403,000 sentences embedded.\n",
      "404,000 sentences embedded.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-18 00:05:19,246 : WARNING : From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/utils/losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-18 00:05:20,818 : WARNING : From /anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/utils/losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 25)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 25)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential (Sequential)         (None, 128)          25976448    input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "manhattan_distance (ManhattanDi (None, 1)            0           sequential[1][0]                 \n",
      "                                                                 sequential[2][0]                 \n",
      "==================================================================================================\n",
      "Total params: 25,976,448\n",
      "Trainable params: 219,648\n",
      "Non-trainable params: 25,756,800\n",
      "__________________________________________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 25, 300)           25756800  \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               219648    \n",
      "=================================================================\n",
      "Total params: 25,976,448\n",
      "Trainable params: 219,648\n",
      "Non-trainable params: 25,756,800\n",
      "_________________________________________________________________\n",
      "Train on 283003 samples, validate on 121287 samples\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-18 00:05:20,990 : WARNING : From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "282880/283003 [============================>.] - ETA: 0s - loss: 0.1775 - acc: 0.7521\n",
      "Epoch 00001: val_loss improved from inf to 0.15933, saving model to /Users/rameshsubedi/model/weights.01.h5\n",
      "283003/283003 [==============================] - 623s 2ms/sample - loss: 0.1775 - acc: 0.7521 - val_loss: 0.1593 - val_acc: 0.7813\n",
      "Epoch 2/50\n",
      "282880/283003 [============================>.] - ETA: 0s - loss: 0.1525 - acc: 0.7920\n",
      "Epoch 00002: val_loss improved from 0.15933 to 0.15051, saving model to /Users/rameshsubedi/model/weights.02.h5\n",
      "283003/283003 [==============================] - 627s 2ms/sample - loss: 0.1525 - acc: 0.7920 - val_loss: 0.1505 - val_acc: 0.7955\n",
      "Epoch 3/50\n",
      "282880/283003 [============================>.] - ETA: 0s - loss: 0.1447 - acc: 0.8042\n",
      "Epoch 00003: val_loss improved from 0.15051 to 0.14625, saving model to /Users/rameshsubedi/model/weights.03.h5\n",
      "283003/283003 [==============================] - 621s 2ms/sample - loss: 0.1447 - acc: 0.8042 - val_loss: 0.1462 - val_acc: 0.7992\n",
      "Epoch 4/50\n",
      "282880/283003 [============================>.] - ETA: 0s - loss: 0.1396 - acc: 0.8121\n",
      "Epoch 00004: val_loss improved from 0.14625 to 0.14329, saving model to /Users/rameshsubedi/model/weights.04.h5\n",
      "283003/283003 [==============================] - 619s 2ms/sample - loss: 0.1396 - acc: 0.8120 - val_loss: 0.1433 - val_acc: 0.8061\n",
      "Epoch 5/50\n",
      "282880/283003 [============================>.] - ETA: 0s - loss: 0.1356 - acc: 0.8187\n",
      "Epoch 00005: val_loss improved from 0.14329 to 0.14129, saving model to /Users/rameshsubedi/model/weights.05.h5\n",
      "283003/283003 [==============================] - 617s 2ms/sample - loss: 0.1356 - acc: 0.8187 - val_loss: 0.1413 - val_acc: 0.8079\n",
      "Epoch 6/50\n",
      "282880/283003 [============================>.] - ETA: 0s - loss: 0.1321 - acc: 0.8240\n",
      "Epoch 00006: val_loss improved from 0.14129 to 0.13952, saving model to /Users/rameshsubedi/model/weights.06.h5\n",
      "283003/283003 [==============================] - 623s 2ms/sample - loss: 0.1321 - acc: 0.8240 - val_loss: 0.1395 - val_acc: 0.8111\n",
      "Epoch 7/50\n",
      "282880/283003 [============================>.] - ETA: 0s - loss: 0.1291 - acc: 0.8288\n",
      "Epoch 00007: val_loss improved from 0.13952 to 0.13804, saving model to /Users/rameshsubedi/model/weights.07.h5\n",
      "283003/283003 [==============================] - 619s 2ms/sample - loss: 0.1290 - acc: 0.8288 - val_loss: 0.1380 - val_acc: 0.8144\n",
      "Epoch 8/50\n",
      "282880/283003 [============================>.] - ETA: 0s - loss: 0.1264 - acc: 0.8333\n",
      "Epoch 00008: val_loss improved from 0.13804 to 0.13665, saving model to /Users/rameshsubedi/model/weights.08.h5\n",
      "283003/283003 [==============================] - 619s 2ms/sample - loss: 0.1264 - acc: 0.8333 - val_loss: 0.1367 - val_acc: 0.8148\n",
      "Epoch 9/50\n",
      "282880/283003 [============================>.] - ETA: 0s - loss: 0.1242 - acc: 0.8368\n",
      "Epoch 00009: val_loss improved from 0.13665 to 0.13567, saving model to /Users/rameshsubedi/model/weights.09.h5\n",
      "283003/283003 [==============================] - 620s 2ms/sample - loss: 0.1242 - acc: 0.8368 - val_loss: 0.1357 - val_acc: 0.8174\n",
      "Epoch 10/50\n",
      "282880/283003 [============================>.] - ETA: 0s - loss: 0.1221 - acc: 0.8408\n",
      "Epoch 00010: val_loss improved from 0.13567 to 0.13470, saving model to /Users/rameshsubedi/model/weights.10.h5\n",
      "283003/283003 [==============================] - 621s 2ms/sample - loss: 0.1221 - acc: 0.8408 - val_loss: 0.1347 - val_acc: 0.8184\n",
      "Epoch 11/50\n",
      "282880/283003 [============================>.] - ETA: 0s - loss: 0.1202 - acc: 0.8432\n",
      "Epoch 00011: val_loss improved from 0.13470 to 0.13401, saving model to /Users/rameshsubedi/model/weights.11.h5\n",
      "283003/283003 [==============================] - 621s 2ms/sample - loss: 0.1202 - acc: 0.8432 - val_loss: 0.1340 - val_acc: 0.8199\n",
      "Epoch 12/50\n",
      "282880/283003 [============================>.] - ETA: 0s - loss: 0.1185 - acc: 0.8466\n",
      "Epoch 00012: val_loss improved from 0.13401 to 0.13382, saving model to /Users/rameshsubedi/model/weights.12.h5\n",
      "283003/283003 [==============================] - 617s 2ms/sample - loss: 0.1185 - acc: 0.8466 - val_loss: 0.1338 - val_acc: 0.8214\n",
      "Epoch 13/50\n",
      "282880/283003 [============================>.] - ETA: 0s - loss: 0.1169 - acc: 0.8491\n",
      "Epoch 00013: val_loss improved from 0.13382 to 0.13340, saving model to /Users/rameshsubedi/model/weights.13.h5\n",
      "283003/283003 [==============================] - 615s 2ms/sample - loss: 0.1169 - acc: 0.8491 - val_loss: 0.1334 - val_acc: 0.8213\n",
      "Epoch 14/50\n",
      "282880/283003 [============================>.] - ETA: 0s - loss: 0.1157 - acc: 0.8512\n",
      "Epoch 00014: val_loss did not improve from 0.13340\n",
      "283003/283003 [==============================] - 617s 2ms/sample - loss: 0.1157 - acc: 0.8512 - val_loss: 0.1336 - val_acc: 0.8197\n",
      "Epoch 15/50\n",
      "282880/283003 [============================>.] - ETA: 0s - loss: 0.1145 - acc: 0.8530\n",
      "Epoch 00015: val_loss improved from 0.13340 to 0.13294, saving model to /Users/rameshsubedi/model/weights.15.h5\n",
      "283003/283003 [==============================] - 617s 2ms/sample - loss: 0.1145 - acc: 0.8530 - val_loss: 0.1329 - val_acc: 0.8206\n",
      "Epoch 16/50\n",
      "282880/283003 [============================>.] - ETA: 0s - loss: 0.1133 - acc: 0.8545\n",
      "Epoch 00016: val_loss improved from 0.13294 to 0.13270, saving model to /Users/rameshsubedi/model/weights.16.h5\n",
      "283003/283003 [==============================] - 618s 2ms/sample - loss: 0.1133 - acc: 0.8545 - val_loss: 0.1327 - val_acc: 0.8215\n",
      "Epoch 17/50\n",
      "282880/283003 [============================>.] - ETA: 0s - loss: 0.1120 - acc: 0.8572\n",
      "Epoch 00017: val_loss improved from 0.13270 to 0.13237, saving model to /Users/rameshsubedi/model/weights.17.h5\n",
      "283003/283003 [==============================] - 621s 2ms/sample - loss: 0.1120 - acc: 0.8572 - val_loss: 0.1324 - val_acc: 0.8228\n",
      "Epoch 18/50\n",
      "282880/283003 [============================>.] - ETA: 0s - loss: 0.1109 - acc: 0.8590\n",
      "Epoch 00018: val_loss did not improve from 0.13237\n",
      "283003/283003 [==============================] - 617s 2ms/sample - loss: 0.1109 - acc: 0.8590 - val_loss: 0.1325 - val_acc: 0.8227\n",
      "Epoch 19/50\n",
      "282880/283003 [============================>.] - ETA: 0s - loss: 0.1098 - acc: 0.8606\n",
      "Epoch 00019: val_loss improved from 0.13237 to 0.13219, saving model to /Users/rameshsubedi/model/weights.19.h5\n",
      "283003/283003 [==============================] - 618s 2ms/sample - loss: 0.1098 - acc: 0.8606 - val_loss: 0.1322 - val_acc: 0.8232\n",
      "Epoch 20/50\n",
      "282880/283003 [============================>.] - ETA: 0s - loss: 0.1087 - acc: 0.8628\n",
      "Epoch 00020: val_loss improved from 0.13219 to 0.13214, saving model to /Users/rameshsubedi/model/weights.20.h5\n",
      "283003/283003 [==============================] - 618s 2ms/sample - loss: 0.1087 - acc: 0.8629 - val_loss: 0.1321 - val_acc: 0.8226\n",
      "Epoch 21/50\n",
      "282880/283003 [============================>.] - ETA: 0s - loss: 0.1076 - acc: 0.8645\n",
      "Epoch 00021: val_loss improved from 0.13214 to 0.13170, saving model to /Users/rameshsubedi/model/weights.21.h5\n",
      "283003/283003 [==============================] - 619s 2ms/sample - loss: 0.1076 - acc: 0.8645 - val_loss: 0.1317 - val_acc: 0.8242\n",
      "Epoch 22/50\n",
      "282880/283003 [============================>.] - ETA: 0s - loss: 0.1065 - acc: 0.8664\n",
      "Epoch 00022: val_loss improved from 0.13170 to 0.13162, saving model to /Users/rameshsubedi/model/weights.22.h5\n",
      "283003/283003 [==============================] - 620s 2ms/sample - loss: 0.1065 - acc: 0.8664 - val_loss: 0.1316 - val_acc: 0.8241\n",
      "Epoch 23/50\n",
      "282880/283003 [============================>.] - ETA: 0s - loss: 0.1051 - acc: 0.8696\n",
      "Epoch 00023: val_loss improved from 0.13162 to 0.13086, saving model to /Users/rameshsubedi/model/weights.23.h5\n",
      "283003/283003 [==============================] - 617s 2ms/sample - loss: 0.1051 - acc: 0.8696 - val_loss: 0.1309 - val_acc: 0.8264\n",
      "Epoch 24/50\n",
      "282880/283003 [============================>.] - ETA: 0s - loss: 0.1041 - acc: 0.8711\n",
      "Epoch 00024: val_loss did not improve from 0.13086\n",
      "283003/283003 [==============================] - 617s 2ms/sample - loss: 0.1041 - acc: 0.8711 - val_loss: 0.1309 - val_acc: 0.8257\n",
      "Epoch 25/50\n",
      "282880/283003 [============================>.] - ETA: 0s - loss: 0.1033 - acc: 0.8726\n",
      "Epoch 00025: val_loss improved from 0.13086 to 0.13017, saving model to /Users/rameshsubedi/model/weights.25.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283003/283003 [==============================] - 620s 2ms/sample - loss: 0.1033 - acc: 0.8726 - val_loss: 0.1302 - val_acc: 0.8271\n",
      "Epoch 26/50\n",
      "282880/283003 [============================>.] - ETA: 0s - loss: 0.1023 - acc: 0.8740\n",
      "Epoch 00026: val_loss did not improve from 0.13017\n",
      "283003/283003 [==============================] - 612s 2ms/sample - loss: 0.1023 - acc: 0.8740 - val_loss: 0.1305 - val_acc: 0.8265\n",
      "Epoch 27/50\n",
      "282880/283003 [============================>.] - ETA: 0s - loss: 0.1014 - acc: 0.8758\n",
      "Epoch 00027: val_loss did not improve from 0.13017\n",
      "283003/283003 [==============================] - 612s 2ms/sample - loss: 0.1014 - acc: 0.8758 - val_loss: 0.1303 - val_acc: 0.8274\n",
      "Epoch 28/50\n",
      "282880/283003 [============================>.] - ETA: 0s - loss: 0.1006 - acc: 0.8769\n",
      "Epoch 00028: val_loss improved from 0.13017 to 0.12990, saving model to /Users/rameshsubedi/model/weights.28.h5\n",
      "283003/283003 [==============================] - 614s 2ms/sample - loss: 0.1006 - acc: 0.8769 - val_loss: 0.1299 - val_acc: 0.8278\n",
      "Epoch 29/50\n",
      "282880/283003 [============================>.] - ETA: 0s - loss: 0.0998 - acc: 0.8782\n",
      "Epoch 00029: val_loss did not improve from 0.12990\n",
      "283003/283003 [==============================] - 615s 2ms/sample - loss: 0.0998 - acc: 0.8782 - val_loss: 0.1305 - val_acc: 0.8270\n",
      "Epoch 30/50\n",
      "282880/283003 [============================>.] - ETA: 0s - loss: 0.0989 - acc: 0.8802\n",
      "Epoch 00030: val_loss did not improve from 0.12990\n",
      "283003/283003 [==============================] - 613s 2ms/sample - loss: 0.0989 - acc: 0.8802 - val_loss: 0.1303 - val_acc: 0.8267\n",
      "Epoch 31/50\n",
      "282880/283003 [============================>.] - ETA: 0s - loss: 0.0982 - acc: 0.8815\n",
      "Epoch 00031: val_loss improved from 0.12990 to 0.12973, saving model to /Users/rameshsubedi/model/weights.31.h5\n",
      "283003/283003 [==============================] - 615s 2ms/sample - loss: 0.0982 - acc: 0.8815 - val_loss: 0.1297 - val_acc: 0.8284\n",
      "Epoch 32/50\n",
      "282880/283003 [============================>.] - ETA: 0s - loss: 0.0973 - acc: 0.8828\n",
      "Epoch 00032: val_loss did not improve from 0.12973\n",
      "283003/283003 [==============================] - 613s 2ms/sample - loss: 0.0973 - acc: 0.8828 - val_loss: 0.1308 - val_acc: 0.8268\n",
      "Epoch 33/50\n",
      "282880/283003 [============================>.] - ETA: 0s - loss: 0.0969 - acc: 0.8836\n",
      "Epoch 00033: val_loss did not improve from 0.12973\n",
      "283003/283003 [==============================] - 613s 2ms/sample - loss: 0.0969 - acc: 0.8836 - val_loss: 0.1303 - val_acc: 0.8278\n",
      "Epoch 34/50\n",
      "282880/283003 [============================>.] - ETA: 0s - loss: 0.0960 - acc: 0.8851\n",
      "Epoch 00034: val_loss did not improve from 0.12973\n",
      "283003/283003 [==============================] - 613s 2ms/sample - loss: 0.0960 - acc: 0.8851 - val_loss: 0.1303 - val_acc: 0.8280\n",
      "Epoch 35/50\n",
      "282880/283003 [============================>.] - ETA: 0s - loss: 0.0954 - acc: 0.8860\n",
      "Epoch 00035: val_loss did not improve from 0.12973\n",
      "283003/283003 [==============================] - 614s 2ms/sample - loss: 0.0954 - acc: 0.8860 - val_loss: 0.1299 - val_acc: 0.8287\n",
      "Epoch 36/50\n",
      "282880/283003 [============================>.] - ETA: 0s - loss: 0.0947 - acc: 0.8869\n",
      "Epoch 00036: val_loss did not improve from 0.12973\n",
      "283003/283003 [==============================] - 614s 2ms/sample - loss: 0.0947 - acc: 0.8869 - val_loss: 0.1299 - val_acc: 0.8290\n",
      "Epoch 37/50\n",
      "282880/283003 [============================>.] - ETA: 0s - loss: 0.0940 - acc: 0.8880\n",
      "Epoch 00037: val_loss did not improve from 0.12973\n",
      "283003/283003 [==============================] - 625s 2ms/sample - loss: 0.0940 - acc: 0.8880 - val_loss: 0.1301 - val_acc: 0.8289\n",
      "Epoch 38/50\n",
      "282880/283003 [============================>.] - ETA: 0s - loss: 0.0935 - acc: 0.8891\n",
      "Epoch 00038: val_loss did not improve from 0.12973\n",
      "283003/283003 [==============================] - 615s 2ms/sample - loss: 0.0935 - acc: 0.8891 - val_loss: 0.1301 - val_acc: 0.8294\n",
      "Epoch 39/50\n",
      "282880/283003 [============================>.] - ETA: 0s - loss: 0.0927 - acc: 0.8905\n",
      "Epoch 00039: val_loss did not improve from 0.12973\n",
      "283003/283003 [==============================] - 616s 2ms/sample - loss: 0.0927 - acc: 0.8905 - val_loss: 0.1298 - val_acc: 0.8290\n",
      "Epoch 40/50\n",
      "282880/283003 [============================>.] - ETA: 0s - loss: 0.0922 - acc: 0.8910\n",
      "Epoch 00040: val_loss did not improve from 0.12973\n",
      "283003/283003 [==============================] - 616s 2ms/sample - loss: 0.0922 - acc: 0.8910 - val_loss: 0.1308 - val_acc: 0.8266\n",
      "Epoch 41/50\n",
      "282880/283003 [============================>.] - ETA: 0s - loss: 0.0916 - acc: 0.8922\n",
      "Epoch 00041: val_loss did not improve from 0.12973\n",
      "283003/283003 [==============================] - 614s 2ms/sample - loss: 0.0916 - acc: 0.8922 - val_loss: 0.1299 - val_acc: 0.8297\n",
      "Epoch 42/50\n",
      "282880/283003 [============================>.] - ETA: 0s - loss: 0.0910 - acc: 0.8932\n",
      "Epoch 00042: val_loss did not improve from 0.12973\n",
      "283003/283003 [==============================] - 614s 2ms/sample - loss: 0.0910 - acc: 0.8932 - val_loss: 0.1299 - val_acc: 0.8295\n",
      "Epoch 43/50\n",
      "282880/283003 [============================>.] - ETA: 0s - loss: 0.0903 - acc: 0.8943\n",
      "Epoch 00043: val_loss did not improve from 0.12973\n",
      "283003/283003 [==============================] - 614s 2ms/sample - loss: 0.0903 - acc: 0.8943 - val_loss: 0.1298 - val_acc: 0.8289\n",
      "Epoch 44/50\n",
      "282880/283003 [============================>.] - ETA: 0s - loss: 0.0897 - acc: 0.8953\n",
      "Epoch 00044: val_loss did not improve from 0.12973\n",
      "283003/283003 [==============================] - 615s 2ms/sample - loss: 0.0897 - acc: 0.8954 - val_loss: 0.1306 - val_acc: 0.8284\n",
      "Epoch 45/50\n",
      "282880/283003 [============================>.] - ETA: 0s - loss: 0.0892 - acc: 0.8962\n",
      "Epoch 00045: val_loss did not improve from 0.12973\n",
      "283003/283003 [==============================] - 616s 2ms/sample - loss: 0.0892 - acc: 0.8962 - val_loss: 0.1309 - val_acc: 0.8282\n",
      "Epoch 46/50\n",
      "282880/283003 [============================>.] - ETA: 0s - loss: 0.0886 - acc: 0.8967\n",
      "Epoch 00046: val_loss did not improve from 0.12973\n",
      "283003/283003 [==============================] - 620s 2ms/sample - loss: 0.0886 - acc: 0.8967 - val_loss: 0.1301 - val_acc: 0.8293\n",
      "Epoch 47/50\n",
      "282880/283003 [============================>.] - ETA: 0s - loss: 0.0880 - acc: 0.8981\n",
      "Epoch 00047: val_loss did not improve from 0.12973\n",
      "283003/283003 [==============================] - 616s 2ms/sample - loss: 0.0880 - acc: 0.8981 - val_loss: 0.1309 - val_acc: 0.8286\n",
      "Epoch 48/50\n",
      "282880/283003 [============================>.] - ETA: 0s - loss: 0.0875 - acc: 0.8991\n",
      "Epoch 00048: val_loss did not improve from 0.12973\n",
      "283003/283003 [==============================] - 615s 2ms/sample - loss: 0.0875 - acc: 0.8991 - val_loss: 0.1302 - val_acc: 0.8296\n",
      "Epoch 49/50\n",
      "282880/283003 [============================>.] - ETA: 0s - loss: 0.0871 - acc: 0.8993\n",
      "Epoch 00049: val_loss did not improve from 0.12973\n",
      "283003/283003 [==============================] - 615s 2ms/sample - loss: 0.0871 - acc: 0.8993 - val_loss: 0.1307 - val_acc: 0.8282\n",
      "Epoch 50/50\n",
      "282880/283003 [============================>.] - ETA: 0s - loss: 0.0866 - acc: 0.9004\n",
      "Epoch 00050: val_loss did not improve from 0.12973\n",
      "283003/283003 [==============================] - 614s 2ms/sample - loss: 0.0866 - acc: 0.9004 - val_loss: 0.1304 - val_acc: 0.8293\n",
      "Training time finished.\n",
      "50 epochs in     30860.13\n",
      "0.8292(max: 0.8297)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    X_train, X_validation, Y_train, Y_validation, embeddings = prepare_data()\n",
    "    model = prepare_model(embeddings)\n",
    "    malstm_trained = train_model(X_train, X_validation, Y_train, Y_validation, model) \n",
    "    plot_accuracy_and_loss(malstm_trained)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conclusion:\n",
    "\n",
    "# 1. Based on Character Level TF-IDF + Xgboost (which does not capture the sentiments of question-pairs) \n",
    "#    we get the best 'recall' of 68%. That is, the proportion of duplicate questions that our model is \n",
    "#    able to detect over the total amount of duplicate questions is 0.68.\n",
    "#    The maximum accuracy obtined is 82.56%\n",
    "\n",
    "# 2. To capture the sentiment of question-pairs, we use word2vec embeddings and employ \n",
    "#    MaLSTM (Manhattan Long Short-Term Memory) neural network for the analysis.\n",
    "#    We find question-pair's duplicate accuracy of 82.97%.\n",
    "\n",
    "# 3. The accuracies obtained from both methods are almost the same.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
